{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e072582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\39324\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "%run setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2a63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\39324\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\39324\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868cf763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:23: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:23: DeprecationWarning: invalid escape sequence \\S\n",
      "C:\\Users\\39324\\AppData\\Local\\Temp\\ipykernel_1752\\1844685082.py:23: DeprecationWarning: invalid escape sequence \\S\n",
      "  p1 = re.sub(pattern='http://\\S+|https://\\S+',repl= '', string=word)\n"
     ]
    }
   ],
   "source": [
    "def text_clean(corpus, keep_list):\n",
    "    '''\n",
    "    Purpose : Function to keep only alphabets, digits and certain words \n",
    "              (punctuations, qmarks, tabs etc. removed)\n",
    "    \n",
    "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, \n",
    "            'keep_list', which have to be retained even after the cleaning process\n",
    "    \n",
    "    Output : Returns the cleaned text corpus\n",
    "    \n",
    "    '''\n",
    "    cleaned_corpus = pd.Series(dtype = 'object')\n",
    "    for row in corpus:\n",
    "        row=html.unescape(row)\n",
    "        qs = []\n",
    "        for word in row.split():\n",
    "            if word=='WHO' or word=='W.H.O' or word=='W.H.O.':\n",
    "                qs.append('WHO')\n",
    "            if word not in keep_list:\n",
    "                #removal of  punctuation, articles, question marks, tabs but also URLS, hashtags\n",
    "                #by re.sub: replacing using regular expressions\n",
    "                #pattern is the pattern the function has to find in the string, and substitute it with repl\n",
    "                #lowering all terms\n",
    "                #urls\n",
    "                p1 = re.sub(pattern='http://\\S+|https://\\S+',repl= '', string=word)\n",
    "                #&amp removal\n",
    "                p1 = re.sub(pattern='&(?!amp;)',repl='',string=p1)\n",
    "                #hashtags\n",
    "                p1 = re.sub(pattern='#[A-Za-z0-9_]+',repl= '', string=p1)\n",
    "                #replace all non letters with spaces\n",
    "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl='', string=p1)\n",
    "                # retwetted usernames\n",
    "                p1 = re.sub(pattern='RT @([a-zA-Z0-9_]{1,50} +)',repl='', string=p1)\n",
    "                #usernames\n",
    "                p1 = re.sub(pattern='@([a-zA-Z0-9_]{1,50} +)',repl='', string=p1)\n",
    "                #punctuation and special characters\n",
    "                p1 = re.sub(pattern=r'[^\\w\\s]',repl='', string=p1)\n",
    "                p1 = p1.lower()\n",
    "                qs.append(p1)\n",
    "            else: qs.append(word)\n",
    "        cleaned_corpus = pd.concat([cleaned_corpus, pd.Series(' '.join(qs))])\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc21a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of words without a concrete meaning: wh words and stopwords in english dictionary represent useless data\n",
    "#stopword are those of the NLTK Stopword List: english dicstionary\n",
    "def stopwords_removal(corpus):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6de955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization with wordnetlemmatizer to normalize the text: removing suffixes to create standardized words\n",
    "#lemma is an actual word, stem is not\n",
    "def lemmatize(corpus):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8841ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(corpus, stem_type = None):\n",
    "    if stem_type == 'snowball':\n",
    "        print('snowball')\n",
    "        stemmer = SnowballStemmer(language = 'english')\n",
    "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    else:\n",
    "        print('stemmer')\n",
    "        stemmer = PorterStemmer()\n",
    "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc284ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, keep_list, cleaning = True, \n",
    "               stemming = False, stem_type = None, \n",
    "               lemmatization = False, \n",
    "               remove_stopwords = True):\n",
    "    '''\n",
    "    Purpose : Function to perform all pre-processing tasks seen until now\n",
    "              (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
    "    \n",
    "    Input : \n",
    "    'corpus'    - Text corpus on which pre-processing tasks will be performed\n",
    "    'keep_list' - List of words to be retained during cleaning process\n",
    "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean \n",
    "                  variables indicating whether a particular task should \n",
    "                  be performed or not\n",
    "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. \n",
    "                  Default is \"None\", which corresponds to Porter\n",
    "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
    "    \n",
    "    Note : Either stemming or lemmatization should be used. \n",
    "           There's no benefit of using both of them together\n",
    "    \n",
    "    Output : Returns the processed text corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if cleaning == True:\n",
    "        corpus = text_clean(corpus, keep_list)\n",
    "    \n",
    "    if remove_stopwords == True:\n",
    "        corpus = stopwords_removal(corpus)\n",
    "    else :\n",
    "        corpus = [[x for x in x.split()] for x in corpus]\n",
    "    \n",
    "    if lemmatization == True:\n",
    "        corpus = lemmatize(corpus)    \n",
    "        \n",
    "    if stemming == True:\n",
    "        corpus = stem(corpus, stem_type)\n",
    "    \n",
    "    corpus = [' '.join(x) for x in corpus]        \n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e2f36",
   "metadata": {},
   "source": [
    "# Functions for top n unigrams, bi-grams, tri-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037dc01",
   "metadata": {},
   "source": [
    "UNIGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2e1cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c2aa5",
   "metadata": {},
   "source": [
    "BIGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ffc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bda742",
   "metadata": {},
   "source": [
    "TRIGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "032b3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5472da",
   "metadata": {},
   "source": [
    "TRANSFORM CORPUS IN A LIST OF UNIGRAMS FOR EVERY DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1eba501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_unigrams(corpus_preprocessed):\n",
    "    words = corpus_preprocessed.split()\n",
    "    unigram_tuples = list(words)\n",
    "    return [unigram_tuples[i] for i in range(len(unigram_tuples))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c5d06",
   "metadata": {},
   "source": [
    "TRANSFORM CORPUS IN A LIST OF BIIGRAMS FOR EVERY DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c159db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bigrams(corpus_preprocessed):\n",
    "    words = corpus_preprocessed.split()\n",
    "    bigram_tuples = list(ngrams(words, 2))\n",
    "    return ['{} {}'.format(bigram_tuples[i][0], bigram_tuples[i][1]) for i in range(len(bigram_tuples))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de470d9",
   "metadata": {},
   "source": [
    "SHOWING TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee6bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(vectorizer, lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names_out())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99457d0d",
   "metadata": {},
   "source": [
    "GET DOMAIN NAME (GET THE NAME OF THE SOURCE FROM AN ANALYSED URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33150b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_name(url):\n",
    "    parsed_uri = urlparse(url)\n",
    "    domain = parsed_uri.netloc\n",
    "    return domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51bce0",
   "metadata": {},
   "source": [
    "SUM OF VALUES OF DICTIONARIES IN A LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_values(word_count):\n",
    "    sum_list = []\n",
    "    for dictionary in word_count:\n",
    "        sum_list.append(sum(dictionary.values()))\n",
    "    return sum_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
